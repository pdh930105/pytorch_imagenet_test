{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((256, 256)),\n",
    "                                transforms.CenterCrop((224, 224)),\n",
    "                              transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class val_imagenet_dataset(Dataset):   \n",
    "    def __init__(self, root=\"../imagenet/\", transform=None):\n",
    "        self.transform = transform\n",
    "        self.root = root\n",
    "        self.data = []\n",
    "        self.target = []\n",
    "        f = open(self.root+\"/ILSVRC2012_validation_ground_truth.txt\")\n",
    "        label_data = f.readlines()\n",
    "        file_list = os.listdir(self.root)\n",
    "        image_list = [image for image in file_list if image.endswith(\".JPEG\")]\n",
    "        image_list.sort()\n",
    "        for idx, label in enumerate(tqdm(label_data)):\n",
    "            self.data.append(image_list[idx])\n",
    "            self.target.append(int(label)-1) \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.data[idx], self.target[idx]\n",
    "        \n",
    "        with open(self.root+path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            sample = img.convert('RGB')\n",
    "\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(model, dataloader, device, criterion =None):\n",
    "    since = time.time()\n",
    "    acc = 0\n",
    "    loss = 0\n",
    "    data_size = dataloader.dataset.__len__()\n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        acc += torch.sum(preds==labels.data)\n",
    "        \n",
    "        if criterion is not None:\n",
    "            _loss = criterion(outputs, labels)\n",
    "            loss +=_loss.item() * inputs.size(0)\n",
    "            print(\"local loss : {:4f}\".format(loss))\n",
    "        \n",
    "    acc = acc.double()/data_size\n",
    "    loss = loss / data_size\n",
    "    \n",
    "    print(\"Acc : {:4f}\".format(acc))\n",
    "    if criterion is not None:\n",
    "        print(\"Loss : {:4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 441639.57it/s]\n"
     ]
    }
   ],
   "source": [
    "val_imagenet = val_imagenet_dataset(transform=transform)\n",
    "val_dataloader = DataLoader(val_imagenet, batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1853/50000 [00:40<15:30, 51.75it/s] "
     ]
    }
   ],
   "source": [
    "resnet_50_origin = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "val_model(resnet_50_origin, val_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
